# Lecture 9 notes
### Stanisław Antonowicz
## Unsupervised learning and spatial transcriptomics


### Recap – neural networks

- Complex models that achieve non-linearity by stacking simple computational components called neurons with non-linear activation functions between them.
- Neural networks are highly parametrized, require a lot of training examples and are considered black boxes – understaing why the model made a particular decision is really hard.
### Unsupervised learning
- The idea is to discover patterns in the data itself, without any labels for each sample.
- `PCA` can be viewed as a most primitive example of this approach – we learn (through a series of simple matrix operations, but still) a lower-dimensional representation of the data that still contains some information that is useful to us. Another popular dimensionality reduction techniques - `t-SNE` and `UMAP` – are also an example of that.
- Clustering techniques, like `K-Means`, `UPMGA` and `DBSCAN`, are also an example of unsupervised learning.

### How to model this?
Conceptually, the data $X$ is generated by some latent variable $Z$. We would like to estimate $Z$.
$$
\begin{align*}
P(Z|X)=\frac{P(X|Z)P(Z)}{P(X)}
\end{align*}
$$

This is impossible to calculate since we would have to integrate over all $Z$.

Instead, we use variational inference technique – we utilize a simpler distribution, $Q$, that is used to approximately model $Z$: $P(Z|X) \approx Q(Z)$. We can compare the distribution using Kullback-Leibler divergence ($\sum_{x \in X} P(x)log\frac{P(x)}{Q(x)}$) and try to minimize, thus making the distributions really similar.

### Encoder and Decoder model (Autoencoder)
<figure>
  <img src="https://www.compthree.com/images/blog/ae/ae.png" alt="Trulli">
  <figcaption>Source: https://www.compthree.com/blog/autoencoder/</figcaption>
</figure>


- The idea is to try encode the data to a lower-dimensional vector and then decode it to an original one.
- This framework can be used in at least two ways:
  - As a dimensionality reduction and feature extraction technique (things like Word2Vec or FastText in NLP use similar approach for word vectors).
  - For generating new samples from latent space – this can be used for all those funny "this person doesn't exist" type of models, but also for generating novel chemical compounds with certain properties and numerous other applications.

The issue with this simple approach is that the model usually doesn't learn that points close in latent space should point to things similar in feature space. This can be alleviated by adding "Variational" to the Autoencoder. In this model, we don't learn latent vectors, but distributions. A latent vector for a particular sample is then computed by sampling from the Gaussian distribution with learned mean and variance. Thanks to this, during training the decoder effectively sees each latent vector with some random errors added, so it generalizes better on unseen latent vectors.

### Spatial transcriptomics

- Data with gene expression in different areas of the tissue. This allows for grouping genes with similar function in a disease together and potentially, given the data with sick and healthy tissues, try to automatically recognize the ones that are malfunctioning/neoplasms etc. Unsupervised learning methods described above can be used to create some latent vectors useful for clustering and visualisation.